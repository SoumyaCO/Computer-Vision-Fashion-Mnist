{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMBncTbWbVRqAZFif+1/V45",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumyaCO/Computer-Vision-Fashion-Mnist/blob/main/Natural_Language_Processing_with_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ—£ï¸ Natural Language Processing with tensorflow\n",
        "\n",
        "Matrials for this courses:\n",
        "\n",
        "* ðŸ”— [Course Link](https://www.coursera.org/learn/natural-language-processing-tensorflow/)\n",
        "* ðŸŒ [Github Repository](https://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main)\n",
        "* ðŸ¤– [Tensorflow Documentation](https://www.tensorflow.org/tutorials/text)\n",
        "\n",
        "### Contents:\n",
        "* Introduction\n",
        "* Word based Encodings\n",
        "* Texts to sequences\n",
        "* Padding\n",
        "* Sarcasm\n",
        "* Assignment[BBC-news archieve]"
      ],
      "metadata": {
        "id": "Qq0eqDRHimHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Introdution: {#introdution}"
      ],
      "metadata": {
        "id": "NV1q9JohlpcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code goes here"
      ],
      "metadata": {
        "id": "sR5T-yofl2eH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word based Encodings"
      ],
      "metadata": {
        "id": "TVial47nlpZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Goes here"
      ],
      "metadata": {
        "id": "6_A0UVDvl5xC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Texts to Sequences"
      ],
      "metadata": {
        "id": "MA0BDo2ilzbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Goes here"
      ],
      "metadata": {
        "id": "zz_Obds4l7bC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Padding"
      ],
      "metadata": {
        "id": "zJ1WCwPklpSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code goes here"
      ],
      "metadata": {
        "id": "QeMXssPymAXL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sarcasm\n",
        "**Data Info**:\n",
        "This is a dataset from kaggle.\n",
        "It contains some news article headlines with their lins and a label determining wheteher the headline is sarcastic.\n",
        "\n",
        "**Goal**: we have to analyze the data and have to predict which one of the heading is sarcastic and which one is not.\n",
        "***\n",
        "\n",
        "**ðŸ”‘ Approach**\n",
        "> * Download the data from ðŸ”— [here](https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json)\n",
        "> * loading the data from json format to a python dictionary format\n",
        "> * storing the **headlines**, **labels** and **links** into seperate python lists\n",
        "> * turn the headline into padded sequences"
      ],
      "metadata": {
        "id": "v9voErsRlpE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset:\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yX1ZHSUlVMh",
        "outputId": "497e90be-e552-4197-ee74-49fe6b5f5133"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-09 14:16:58--  https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.207, 142.251.175.207, 74.125.24.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5643545 (5.4M) [application/json]\n",
            "Saving to: â€˜sarcasm.json.1â€™\n",
            "\n",
            "sarcasm.json.1      100%[===================>]   5.38M  4.86MB/s    in 1.1s    \n",
            "\n",
            "2023-09-09 14:17:00 (4.86 MB/s) - â€˜sarcasm.json.1â€™ saved [5643545/5643545]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import JSON and load the file in a seperate file:\n",
        "import json\n",
        "\n",
        "# Load json file\n",
        "with open(\"./sarcasm.json\", 'r') as f: # the r parameter is to open the file in read-only mode\n",
        "    datastore = json.load(f)"
      ],
      "metadata": {
        "id": "3Ipc_n9Llcm8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-sarcastic headline\n",
        "print(f\"Not Sarcastic:{datastore[0].get('headline')}\")\n",
        "\n",
        "# sarcastic headline\n",
        "print(f\"Sarcastic:{datastore[20000].get('headline')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwzdE7cinAa1",
        "outputId": "ace3e56c-f14b-4372-d437-56ee99caffb6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not Sarcastic:former versace store clerk sues over secret 'black code' for minority shoppers\n",
            "Sarcastic:pediatricians announce 2011 newborns are ugliest babies in 30 years\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "Collect the headlines and labels and URLs, though URLs are of no use in this problem scope.\n",
        "***"
      ],
      "metadata": {
        "id": "0blQn8ctnRfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the lists\n",
        "headlines = []\n",
        "labels = []\n",
        "# though not needed but we're storing the links as well\n",
        "links = []\n",
        "\n",
        "for item in datastore:\n",
        "    headlines.append(item.get('headline'))\n",
        "    labels.append(item.get('is_sarcastic'))\n",
        "    # links:\n",
        "    links.append(item.get('article_link'))"
      ],
      "metadata": {
        "id": "AGv_Y3b5ojKv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "We'll now convert the list of sentences into padded sequences.\n",
        "The code cell below generates a dictionary of all all the unique words with values (numerical) and also a list of padded sequences for each of the 26,709 headlines"
      ],
      "metadata": {
        "id": "8h3TgkmypOIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libarary\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initializing tokenizer\n",
        "tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
        "\n",
        "# Generate the word_index dictionary\n",
        "tokenizer.fit_on_texts(headlines)\n",
        "\n",
        "# length of the word index\n",
        "word_index = tokenizer.word_index\n",
        "print(f\"number of words in word index: {len(word_index)}\")\n",
        "\n",
        "# word index\n",
        "# print(f\"word index: {word_index}\")\n",
        "# print()\n",
        "\n",
        "# getting seqeunces (list) from the headlines\n",
        "sequences = tokenizer.texts_to_sequences(headlines)\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "\n",
        "# Print a sample headline\n",
        "index = 2\n",
        "print(f\"sample headline: {headlines[index]}\")\n",
        "print(f\"padded sequence: {padded[index]}\")\n",
        "\n",
        "# Print dimensions of the padded sequences\n",
        "print(f\"shape of the paddded sequences: {padded.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU8GOlShpo9z",
        "outputId": "d15d1566-2678-4668-9d8f-ec4f7871e5fc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of words in word index: 29657\n",
            "sample headline: mom starting to fear son's web series closest thing she will have to grandchild\n",
            "padded sequence: [  145   838     2   907  1749  2093   582  4719   221   143    39    46\n",
            "     2 10736     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "shape of the paddded sequences: (26709, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "# <font color='orange'>ASSIGNMENT:1 Explore the BBC News Archieve</font>\n",
        "\n",
        "**ðŸ’¡ <u>Data Info</u>**:\n",
        "Data contains heading from BBC news archieve, the headings are in one of the 5 categories: -\n",
        "* Business\n",
        "* Entertainment\n",
        "* Politics\n",
        "* Sport\n",
        "* Tech\n",
        "each headline has a label defining to which category the headline belongs\n",
        "\n",
        "**ðŸŽ¯ <u>Goal</u>**: To classify a article/heading based on its category by understanding its relevance and topic.\n",
        "\n",
        "**ðŸ”‘  <u>Approach:</u>**\n",
        "> *"
      ],
      "metadata": {
        "id": "zzzYkQ0FsRZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THESE CELLS AFTER RECONNECTING...\n",
        "# FROM HERE ==================================================\n",
        "# Download the data from kaggle:\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "metadata": {
        "id": "h9AJ7oSZxrtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating kaggle config.\n",
        "# !mkdir ~/.kaggle\n",
        "# !mv kaggle.json ~/.kaggle/\n",
        "# !chmod u+x ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "km-8VAepz08_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the data as zip file\n",
        "!kaggle competitions download -c learn-ai-bbc"
      ],
      "metadata": {
        "id": "uCcx3CZx0KSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzipping the file from\n",
        "!unzip learn-ai-bbc.zip -d data\n",
        "\n",
        "# TO HERE ======================================================"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVemwkmK0gph",
        "outputId": "fd26981f-6221-4511-f0f7-d2160fb749e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  learn-ai-bbc.zip\n",
            "  inflating: data/BBC News Sample Solution.csv  \n",
            "  inflating: data/BBC News Test.csv  \n",
            "  inflating: data/BBC News Train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the data and get started."
      ],
      "metadata": {
        "id": "zcunhcg402H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT MODULES\n",
        "import csv\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "zjN3hBrm1Nin"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOOKING AT THE STRUCTURE\n",
        "training_data_path = os.path.join('./data', \"BBC News Train.csv\")\n",
        "with open(training_data_path, 'r') as csvfile:\n",
        "    print(f\"first line(header) looks like this:\\n\\n{csvfile.readline()}\")\n",
        "    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPIldI_c1cbz",
        "outputId": "556b589e-c0a8-4c30-f7ae-62db5501fd21"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first line(header) looks like this:\n",
            "\n",
            "ArticleId,Text,Category\n",
            "\n",
            "Each data point looks like this:\n",
            "\n",
            "1833,worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (Â£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.,business\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remove stopwords\n",
        "**Stopwords** are the most common words in the language and they rarely provide useful information for the classification process"
      ],
      "metadata": {
        "id": "-he7gysL2BFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE STOPWORD FUNCTION\n",
        "def remove_stopwords(sentence):\n",
        "    \"\"\"\n",
        "    removes a list of stopwords\n",
        "\n",
        "    Args:\n",
        "        sentence (string): sentence to remove the stopwords from\n",
        "    Returns:\n",
        "        sentence (string): lowercase sentence without stopwords\n",
        "    \"\"\"\n",
        "    # List of stopwords\n",
        "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"i\", \"you\", \"to\", \"the\"]\n",
        "\n",
        "\n",
        "    # sentence converted to lowercase\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # removing the stopwords\n",
        "    words = []\n",
        "    for word in sentence.split(\" \"):\n",
        "        if word not in stopwords:\n",
        "            words.append(word)\n",
        "    sentence = \" \".join(words)\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "r-DUqM5g3sBV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING THE FUNCTION\n",
        "remove_stopwords(\"I am about to go to the store and get any snack\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gvvv0L3-5M0z",
        "outputId": "261b670d-d6bd-45be-bc6c-1f80e837ec53"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go store get snack'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading the raw data\n",
        "Now we have to read the data from the csv file. <font color='orange'>To do so - </font>\n",
        "> * Omit the first line at it contains the headers and not data points\n",
        "> * No need to store the datapoints as **numpy arrays** regular list is fine\n",
        "> * to read from csv file use `csv.reader` by passing the appropeate arguments\n",
        "> `csv.reader` returns an iterable that returns each row in every iteration, So the label can be accessed via `row[0]` and the text via `row[1]`\n",
        "> Use `remove_stopwords` function for each sentences"
      ],
      "metadata": {
        "id": "cCErpwVd5oUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARSE DATA FROM FILE:\n",
        "def parse_data_from_file(filename: str)->tuple:\n",
        "    \"\"\"\n",
        "    Extracts sentences and labels from a CSV file\n",
        "\n",
        "    Args:\n",
        "        filename (string): path to the CSV file\n",
        "    Returns:\n",
        "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter = ',')\n",
        "        next(reader) # skip a row\n",
        "        for article_info in reader:\n",
        "            sentences.append(remove_stopwords(article_info[1])) # article was in the 2nd index of the list\n",
        "            labels.append(article_info[2]) # article label is in the third index of the list\n",
        "    return sentences, labels"
      ],
      "metadata": {
        "id": "z74Cxr4V9nAV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "# With original\n",
        "sentences, labels = parse_data_from_file(training_data_path)\n",
        "\n",
        "print(\"ORIGINAL DATASET:\\n\")\n",
        "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
        "print(f\"First sentence has {len(labels)} labels in the dataset.\\n\")\n",
        "print(f\"The first 5 labels are {labels[:5]}\\n\\n\")\n",
        "\n",
        "# With a mniature version of the dataset that contains only first 5 rows\n",
        "# mini-sentences, mini_labels = parse_data_from_file()\n",
        "# I don't have the mini batch data.\n",
        "# We'll see"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgI3pJLDJUi6",
        "outputId": "6243f8c7-6503-4f14-8f00-1db382d7ebdb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL DATASET:\n",
            "\n",
            "There are 1490 sentences in the dataset.\n",
            "\n",
            "First sentence has 1490 labels in the dataset.\n",
            "\n",
            "The first 5 labels are ['business', 'business', 'business', 'tech', 'business']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Tokenizer"
      ],
      "metadata": {
        "id": "3HV-TX4gOS4r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfx5C0X_S5az",
        "outputId": "36f83bd9-3841-40bc-9e9f-9ec4e9058e5b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7HnegYUPS9uq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}